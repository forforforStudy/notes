# 第一章 什么是深度学习

从大到小的关系应该是：人工智能 -> 机器学习 -> 深度学习

即人工智能中包含深度机器学习，机器学习中包含深度学习；深度学习是机器学习中的一种.

## 符号主义人工智能

> 只要程序员精心编写足够多的明确规则来处理知识，就可以实现与人类水平相当的人工智能

类似我们使用的【智能出题】是一种典型的符号主义人工智能，同规明确规则来产题，也是一种人工智能的方案。

**经典的程序设计：规则 + 数据 = 答案**

## 机器学习

对于很多没有明确规则的复杂难题，例如：图像分类，语音识别合语言翻译等，就需要机器学习的方式来应用人工智能。

**机器学习：数据 + 答案 = 规则1**

机器学习一定是训练出来的，而不是程序编写出来的。

### 机器学习的三个要素

* 输入数据点
* 预期输出的示例
* 衡量算法效果好坏的方法

来些晦涩的概念：

* 学习：寻找更好数据表示的自动搜索过程
* 机器学习：在预先定义好的可能性空间中，利用反馈信号的指引来寻找输入数据的有用表示

深度学习的重点在于**深度**两个字，**强调从连续的层(`layer`)中进行学习，这些层会对应于越来越有意义的表示**

深度学习中，这些*layer*几乎总是同规叫做*神经网络*的模型来学习得到的。

书中提到了一个对深度学习中的*layer*很有趣的解释：**将深度网络看作多级信息蒸馏操作：信息穿过连续的过滤器，其纯度越来越高（即对任务的帮助越来越大）。**

### 工作原理概述

首先还是要引入晦涩的部分名词定义：

#### 权重

神经网络中每层对输入数据所做的具体操作的载体，权重有时候也曾为该层的**参数**；*（在其它地方学习到的，这应该就是特征值？）*
 
该语境下*学习*的意思是为：神经网络的所有层找到一组权重值，使得该网络能够将每个示例输入与其目标正确对应。

#### 损失函数（代价函数）

衡量输出与预期值之间的距离，有了损失函数才能观察到神经网络的输出以及衡量该网络在这里示例上的效果好坏。

#### 优化器

深度学习的技巧是：利用损失函数的距离值作为反馈信号来对权重值进行微调，以降低当前神经网络的输出与期望的示例之间的偏差。

这种调节使用的就是 **优化器** 来完成， 它基于**反向传播算法**

通过不断的训练，使得损失值尽可能小的网络，其输出值与目标值尽可能接近，这就是训练好的网络。

# 第二章 深度学习所用的数学基础

## 张量 `tensor`

一般而言当前的所有机器学习系统都是用**张量**作为基本的数据结构

这里给出**张量**的定义：*数据容器，数字的容器。它是矩阵向任意维度的推广，而张量的维度通常称作**轴(axis)***

## 标量（0D张量，零维张量）

即零维张量， `axis === 0`

零维张量的代码：

```python
import numpy as np
x = np.array(12)

>>>x
array(12)
>>>x.ndim
0
```

## 向量（1D张量，一维张量）

一维张量的代码：

```python
x = np.array([12, 3, 6, 14, 7])

>>>x
array([12, 2, 6, 14, 7])
>>>x.ndim
1
```

## 矩阵（2D张量，二维张量）

二维张量代码：

```python
x = np.array([[5, 78, 2, 34, 0],
              [6, 79, 3, 35, 1],
              [7, 80, 4, 36, 2]])

>>> x.ndim
2
```

## 3D张量与更高维的张量

```python
x = np.array([
              [
                [5, 78, 2, 34, 0],
                [6, 79, 3, 35, 1],
                [7, 80, 4, 36, 2]
              ],
              [
                [5, 78, 2, 34, 0],
                [6, 79, 3, 35, 1],
                [7, 80, 4, 36, 2]
              ],
              [
                [5, 78, 2, 34, 0],
                [6, 79, 3, 35, 1],
                [7, 80, 4, 36, 2]
              ]
            ])
>>> x.ndim
3
```

## 张量的三个关键属性

### 轴（阶）

可以认为是数组最外层是几阶数组， 例如3D张量3个轴，矩阵2个轴。*在python中就是`ndim`*

### 形状

表示张量沿着每个轴展开的维度大小（元素个数），例如：

* `(3, 5)` 表示矩阵的形状， 第一阶数组为3个，第二阶数组为5个 *（见：矩阵（2D张量，二维张量））*
* `(3, 3, 5)`表示3D张量，第一阶数组为3个，第二阶为3个，第三阶为5个 *（见：3D张量与更高维的张量）*

### 数据类型

即张量中每个元素的类型，常见的比如：`float32, uint8, float64`等

以书中给的例子为例：
```python
>>> print(train_images.shape)

(60000, 28, 28)

>>> print(train_images.dtype)

unit8
```

表示的是8位整数组成的3D张量，准确的是 60000 个矩阵数组组成的数组每个矩阵由 28*28 个整数组成的二维数组

## Numpy中的张量操作

### 张量切片

通过numpy基于python的语法进行张量的切片, 选取第 10 ~ 100 项

```python
>>> my_slice = train_images[10:100]
>>> print(my_slice.shape)

(90, 28, 28)

// 下述的操作方案相同

>>> my_slice = train_images[10:100, :, :]
>>> my_slice.shape

(90, 28, 28)

>>> my_slice = train_images[10:100, 0:28, 0:28]
>>> my_slice.shape

(90, 28, 28)
```

### 数据批量

深度学习中的所有数据张量的第一个轴（0轴）都是**样本轴**

深度学习中，一般而言不会同时处理整个数据集，因为整个数据集过于庞大，因此更多的是**将数据拆分位小的批量**，本书中给出例子：

```python
# 第一批
batch = train_images[:128]
# 第二批
batch02 = train_images[128:256]

# 第n批 ...
```

### 常见的数据张量

* 向量数据：2D张量 `(samples, features)`

一般而言，第一个轴是**样本轴**，第二个轴是**特征轴**。例如人口统计：数据集包含10000人，每个人包含年龄，邮编以及收入3个特征值向量，那么这个数据的形状就是(10000, 3)

* 时间序列数据或序列数据：3D张量 `(samples, timesteps, features)`

对时间敏感的数据，则将数据存储在带有时间轴的3D张量中。每个样本可以被编码位一个向量序列（即2D张量）

* 图像：4D张量 `(samples, height, width, channels) or (samples, channels, height, width)`

例如图像大小位256x256的128张灰度图像的形状应该是：**(128, 256, 256, 1)**（灰度图像的彩色通道已有一维）

* 视频：5D张量 `(samples, frames, height, width, channels) or (samples, frames, channels, height, width)`

举个例子，以一个每秒4帧采样的60秒YouTube视频片段，视频尺寸为 144x256，这个视频总共又240帧；4个这样的视频片段组成的批量将保存在形状为 (4, 240, 144, 256, 3)的张量中，**其中3表示的是3个的彩色通道图像**

## 张量运算

深度神经网络学到的所有变化也都可以简化为数值数据张量上的一些**张量运算**，通过**张量运算**输入一个张量输出另一个张量

### 逐元素运算

逐元素的运算独立地应用于张量的每一个元素，非常适合大规模的并行实现。

书中给出了一个逐个执行 `max(x, 0)` 的python实现代码：

```python
def naive_relu(x):
  assert len(x.shape) == 2
  x = x.copy()
  for i in range(x.shape[0]):
    for j in range(x.shape[1]):
      x[i, j] = max(x[i, j], 0)
  return x
```

上述代码其实就是一个2D张量循环方式的遍历执行 `max(x[i, j], 0)`

其实在 `numpy` 中对张量的各种操作都已经优化好的内置函数了，例如2个同shape的张量相加，逐元素的`relu //即 max(x, 0)`:

```python
import numpy as np

# x.shape == y.shape
z = x + y

z = np.maximum(z, 0.)
```

### 广播

如果两个形状不同的张量相加，在没有歧义的情况下较小的张量会被**广播**，以匹配较大的张量的形状。**广播**的步骤包含如下几步：

1. 向较小的张量添加轴（叫做**广播轴**），使其`ndim`与较大的张量相同
2. 将较小的张量沿着新轴重复，使其形状与较大的张量相同

从书中的例子给出的，似乎能广播的两个张量相加，较小的那个张量的**形状**应该是较大张量的子集：

> 一个张量的形状是 (a, b, ... n, n+1, ... m) ，另一个张量的形状是 (n, n+1,... m) ，那么你通常可以利用广播对它们做两个张量之间的逐元素运算。广播操作会动应用于从 a 到 n-1 的轴

### 张量点积

也叫做**张量积，最常见的张量运算**

```python
import numpy as np

z = np.dot(x, y)
```

数学中则是: `z=x.y`